{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0760aad",
   "metadata": {},
   "source": [
    "## 카트폴 예: DQN (Deep Q-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eee350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전연결 신경망(fully connected nn) 구성 클래스\n",
    "class FCQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,               # 입력 차원\n",
    "                 output_dim,              # 출력 차원\n",
    "                 hidden_dims=(32,32),     # 은닉계층\n",
    "                 activation_fc=F.relu):   # 활성화 람수\n",
    "        super(FCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc    \n",
    "\n",
    "        # 입력계층->은닉계층1\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        # 은닉계층\n",
    "        self.hidden_layers = nn.ModuleList()    # 신경망 모듈 리스트 초기화\n",
    "\n",
    "        # 은닉계층-1 개 추가\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])  # 은닉계층 추가\n",
    "            self.hidden_layers.append(hidden_layer)   # 모듈 리스트에 추가\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)   # 마지막 은닉계층->출력계층\n",
    "\n",
    "        device = \"cpu\"     \n",
    "        # CUDA(GPU) 가용하면 GPU로 디바이스 설정\n",
    "        if torch.cuda.is_available():    \n",
    "            device = \"cuda:0\"\n",
    "        # 신경망 계산 디바이스 설정\n",
    "        self.device = torch.device(device)  \n",
    "        self.to(self.device)    # 모듈의 파라미터/버퍼 등을 해당 디바이스로 이동(캐스트)\n",
    "        \n",
    "    # 파이토치 텐서가 아니면 텐서로 변환\n",
    "    def _format(self, state):\n",
    "        x = state        \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)    # 첫번째 차원 추가\n",
    "        return x\n",
    "\n",
    "    # 신경망 순전파 함수\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)    # 턴서로 변환\n",
    "        x = self.activation_fc(self.input_layer(x))    # 입력계층\n",
    "        for hidden_layer in self.hidden_layers:        # 은닉계층들\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)        # 출력계층\n",
    "        return x    \n",
    "    \n",
    "    # 넘파이 경험 튜플을 텐서로 변환\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences  \n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)               # 상태\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)              # 행동\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)       # 다음 상태\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)             # 보상\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)   # 종료 상태 여부\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 입실론 그리디 행동 선택 클래스, 기하급수적으로 감가되는 입실론\n",
    "class EGreedyExpStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_epsilon = min_epsilon\n",
    "        # 0.01/[10**-2, .......... 10**0] - 0.01\n",
    "        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
    "        # init_epsilon와 min_epsilon 사이 값으로 조정\n",
    "        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
    "        self.t = 0            # 현재 스텝의 입실론 지정 인덱스\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    # 입실론 선택하고 인덱스 증가\n",
    "    def _epsilon_update(self):\n",
    "        # 미리 감가 계산된 입실론들 중 선택\n",
    "        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]\n",
    "        self.t += 1\n",
    "        return self.epsilon\n",
    "\n",
    "    # 감가된 입실론 그리디 전략으로 예측 신경망 사용하여 행동 선택\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():   # 순전파 예측, 연산 기록 해제\n",
    "            # 신경망 예측 Q 함수 값을 (기울기 연산 기록 해제 후) 넘파이로 변환 후 미니배치 차원 제거\n",
    "            q_values = model(state).detach().cpu().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:   # 최대 Q-함수 행동 선택(그리디)\n",
    "            action = np.argmax(q_values)        \n",
    "        else:                                 # 입실론 확률로 탐험 \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self._epsilon_update()   # 입실론 갱신(감가)\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)    # 탐험 여부 표시\n",
    "        return action    # 선택된 행동 리턴\n",
    "    \n",
    "\n",
    "# 평가 그리디 행동 선택 클래스, 학습된 내용 평가 시에는 그리디 전략 사용하여 행동 선택\n",
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        self.exploratory_action_taken = False\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():    # 순전파 예측, 연산 기록 해제\n",
    "            # 신경망 예측 Q 함수 값을 (기울기 연산 기록 해제 후) 넘파이로 변환 후 미니배치 차원 제거\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "        return np.argmax(q_values)       # 최대 Q-함수 행동 선택(그리디)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3086d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현 버퍼 (Replay Buffer)\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size=10000,   # 버퍼 최대 크기\n",
    "                 batch_size=64):   # 배치 크기\n",
    "        # 저장되는 경험 튜플 초기화\n",
    "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)     # 상태\n",
    "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)     # 행동\n",
    "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)     # 보상\n",
    "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)     # 다음 상태\n",
    "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)     # 종료 여부\n",
    "        # 변수 초기화 \n",
    "        self.max_size = max_size        # 최대 크기\n",
    "        self.batch_size = batch_size    # 배치 크기\n",
    "        self._idx = 0                   # 인덱스\n",
    "        self.size = 0                   # 버퍼 크기\n",
    "    \n",
    "    # 새로운 샘플 저장\n",
    "    def store(self, sample):\n",
    "        # 경험 튜풀 샘플 저장\n",
    "        s, a, r, p, d = sample         # 경험 튜플\n",
    "        self.ss_mem[self._idx] = s     # 상태\n",
    "        self.as_mem[self._idx] = a     # 행동\n",
    "        self.rs_mem[self._idx] = r     # 보상\n",
    "        self.ps_mem[self._idx] = p     # 다음 상태\n",
    "        self.ds_mem[self._idx] = d     # 종료 여부\n",
    "        # 인덱스 증가\n",
    "        self._idx += 1                          \n",
    "        self._idx = self._idx % self.max_size    \n",
    "        # 버퍼 크기 증가\n",
    "        self.size += 1\n",
    "        self.size = min(self.size, self.max_size)\n",
    "\n",
    "    # 배치 크기로 샘플링\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "        # 배치 크기 단위로 id 랜덤 선택\n",
    "        idxs = np.random.choice(self.size, batch_size, replace=False)\n",
    "        # 샘플링된 id로 재현 버퍼에서 경험 튜플 추출\n",
    "        experiences = np.vstack(self.ss_mem[idxs]), \\\n",
    "                      np.vstack(self.as_mem[idxs]), \\\n",
    "                      np.vstack(self.rs_mem[idxs]), \\\n",
    "                      np.vstack(self.ps_mem[idxs]), \\\n",
    "                      np.vstack(self.ds_mem[idxs])\n",
    "        return experiences\n",
    "\n",
    "    # 버퍼의 크기를 리턴, 매직 메서드\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fccacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN 에이전트 클래스\n",
    "class DQN():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,             # 재현 버퍼 람다함수\n",
    "                 value_model_fn,               # 신경망 구성 람다 함수\n",
    "                 value_optimizer_fn,           # 신경망 최적화기법 람다함수\n",
    "                 value_optimizer_lr,           # 학습 속도\n",
    "                 training_strategy_fn,         # 훈련 행동 선택 전략\n",
    "                 evaluation_strategy_fn,       # 평가 행동 선택 전략\n",
    "                 n_warmup_batches,             # 워밍업 미니 배치 수, 샘플링 위한 버퍼의 최소 샘플 수 지정에 사용\n",
    "                 update_target_every_steps):   # 타겟 신경망 갱신 스텝 수\n",
    "        # 에이전트 클래스 속성 초기화\n",
    "        self.replay_buffer_fn = replay_buffer_fn       # 재현 버퍼 람다 함수           \n",
    "        self.value_model_fn = value_model_fn                     \n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "        self.n_warmup_batches = n_warmup_batches            # 워밍업 미니배치 수\n",
    "        self.update_target_every_steps = update_target_every_steps    # 타겟 신경망 갱신 스텝\n",
    "        #self.render = False                                     \n",
    "\n",
    "    # 신경망 학습 모델: 타겟 예측(신경망 예측), 손실함수 계산, 역전파 수행(신경망 파라미터 갱신)\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        # 타겟 신경망 사용하여 다음 상태 Q-함수 구하고 타겟 계산\n",
    "        max_a_q_sp = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        # 온라인 신경망 사용하여 현재 상태 Q-함수 예측\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        # 손실함수 계산\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        # 기울기 계산 (역전파)\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    # 입실론 그리디 전략 사용 행동 선택 후 수행하고 다음 상태 전이\n",
    "    def interaction_step(self, state, env):\n",
    "        # 행동 선택\n",
    "        action = self.training_strategy.select_action(self.online_model, state)\n",
    "        # 행동 수행, 상태 전이\n",
    "        new_state, reward, is_terminal, truncated, info = env.step(action)\n",
    "        # 경험 튜플 재현 버퍼에 저장\n",
    "        experience = (state, action, reward, new_state, float(is_terminal))\n",
    "        self.replay_buffer.store(experience)\n",
    "        \n",
    "        self.episode_reward[-1] += reward       # 보상 추가\n",
    "        self.episode_timestep[-1] += 1          # 타임 스텝 증가\n",
    "        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)    # 탐험 횟수  증가\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    # 온라인 신경망 파라미터 사용하여 타겟 신경망 갱신\n",
    "    def update_network(self):\n",
    "        for target, online in zip(self.target_model.parameters(), \n",
    "                                  self.online_model.parameters()):\n",
    "            target.data.copy_(online.data)\n",
    "\n",
    "    # 신경망 훈련\n",
    "    def train(self, env, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "        # 초기화\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma        \n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)   # 랜덤 시드    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n       # 상태/행동 수\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        # 타겟/온라인 신경망 객체 생성\n",
    "        self.target_model = self.value_model_fn(nS, nA)\n",
    "        self.online_model = self.value_model_fn(nS, nA)\n",
    "        self.update_network()        # 타겟/온라인 신경망 초기화\n",
    "        # 온라인 신경망의 최적화 기법, 재현 버퍼, 행동선택 전략 객체 생성\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n",
    "                                                       self.value_optimizer_lr)\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn()\n",
    "        self.evaluation_strategy = evaluation_strategy_fn() \n",
    "        # 에피소드 당 출력 값 저장 리스트 초기화\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        \n",
    "        # 에피소드 반복\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()        # 에피소드 시작 시간\n",
    "            # 상태, 통계 변수 초기화\n",
    "            state, is_terminal = env.reset(seed=self.seed), False\n",
    "            state = state[0]               # state => (0, {'prob': 1})\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            # 종료 상태 도달 시까지 반복\n",
    "            while not is_terminal:                  \n",
    "                #if self.render:    # 카트폴 동작 렌더링 설정되었으면 렌더링\n",
    "                #    env.render() \n",
    "                # 행동 수행하고 상태 전이\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "                # 샘플링을 위한 최소 재현 버퍼 크기 지정\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                # 재현 버퍼에서 샘플링된 미니배치 데이터 사용하여 신경망 최적화\n",
    "                if len(self.replay_buffer) > min_samples:  \n",
    "                    experiences = self.replay_buffer.sample()          # 재현 버퍼에서 샘플링\n",
    "                    experiences = self.online_model.load(experiences)  # 넘파이 경험 튜플을 텐서로 변환\n",
    "                    self.optimize_model(experiences)      # 신경망 최적화(역전파 수행하여 파라미터 갱신)\n",
    "                # 지정된 타임 스텝(10) 마다 타겟 신경망 갱신\n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_network()\n",
    "                \n",
    "                #if is_terminal:   # 종료 상태  \n",
    "                 #   break\n",
    "            \n",
    "            # 에피소드 당 통계 출력 \n",
    "            episode_elapsed = time.time() - episode_start      # 경과시간\n",
    "            self.episode_seconds.append(episode_elapsed)       # 경과시간 리스트에 추가\n",
    "            training_time += episode_elapsed                   # 훈련시간  \n",
    "            total_step = int(np.sum(self.episode_timestep))    # 누적 타임 스텝\n",
    "\n",
    "            # 한 에피소드 훈련 종료 후 그리디 행동 선택 적용하여 에피소드의 누적 보상 계산\n",
    "            evaluation_score, _ = self.evaluate(self.online_model, env)                        \n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            # 마지막 10/100 스텝 평균 훈련 보상/표준편차\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            # 마지막 1100 스텝 평균 평가 보상/표준편차\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            # 마지막 100스텝 탐험 수/표준편차 출력\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            # 경과 시간 계산\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            \n",
    "            # 에피소드 당 결과 값 리스트에 저장\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            LEAVE_PRINT_EVERY_N_SECS = 60     # 행 출력 지속 시간\n",
    "            ERASE_LINE = '\\x1b[2K'            # 행 삭제                             \n",
    "              \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS  # 디버그 출력 지속시간 초과 플래그 설정\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60      # 최대 제한 시간 초과 플래그 설정\n",
    "            reached_max_episodes = episode >= max_episodes                # 최대 제한 에피소드 수 초과 플래그 설정\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward     # 에피소드 마지막 100스텝 최대 보상 초과 플래그 설정\n",
    "            # 훈련 중지 플래그 설정: 최대 수행 시간이나 에피소드 초과 시 또는 마지막 100 스텝 평균 최대 보상 도달 시\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "\n",
    "            # 에피소드 출력 메시지: 경과시간, 에피소드 번호, 스텝 수, 마지막 10/100 스텝 평균 훈련 보상/표준편차,\n",
    "            #                        마지막 100 스텝 평균 탐험 수/표준편차, 마지막 100 스텝 평균 평가 보상/표준편차\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            \n",
    "            # 디버그 출력 시간 초과 또는 훈련 종료 시 출력 메시지 화면 출력\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            # 훈련 종료 시 출력\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        # 훈련 종료 후 모델 평가 수행\n",
    "        # final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
    "        \n",
    "        final_eval_score, score_std = 0, 0    # ???\n",
    "    \n",
    "        wallclock_time = time.time() - training_start   # 총 훈련 시간(모델 평가 포함)\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    # 에피소드/훈련 종료 후 그리디 전략으로 보상 계산\n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, done = eval_env.reset(), False\n",
    "            s = s[0]\n",
    "            rs.append(0)\n",
    "            while not done:\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s) # 그리디 전략으로 행동 수행하고 상태 전이\n",
    "                s, r, done, _, _ = eval_env.step(a)\n",
    "                rs[-1] += r      # 보상 추가\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    #  힉습 후 그리디 전략으로 렌더링\n",
    "    def render_after_train(self, r_env, n_episodes=1):\n",
    "        for _ in range(n_episodes):\n",
    "            s, done = r_env.reset(), False\n",
    "            s = s[0]\n",
    "            while not done:\n",
    "                r_env.render()     # 렌더링\n",
    "                a = self.evaluation_strategy.select_action(self.online_model, s) # 그리디 전략으로 행동 수행하고 상태 전이\n",
    "                s, r, done, _, _ = r_env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN 메인 루틴\n",
    "dqn_results = []\n",
    "\n",
    "# 각기 다른 시드 값으로 수행 \n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "for seed in SEEDS:\n",
    "    # 환경 세팅 파라미터\n",
    "    environment_settings = {\n",
    "        'env_name': 'CartPole-v1',      # 환경 이름\n",
    "        'gamma': 1.00,                  # 감가율(할인율)\n",
    "        'max_minutes': 20,              # 최대 수행 시간\n",
    "        'max_episodes': 10000,          # 최대 에피소드 수\n",
    "        'goal_mean_100_reward': 475     # 마지막 100 스텝 평균 최대 보상\n",
    "    }\n",
    "    \n",
    "    # 신경망 모델 구성 람다 함수\n",
    "    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n",
    "    # 신경망 최적화 기법 지정 람다 함수\n",
    "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005      # 학습 속도\n",
    "\n",
    "    # 훈련 입실론 그리디 탐험 지정 람다 함수\n",
    "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n",
    "                                                      min_epsilon=0.3, \n",
    "                                                      decay_steps=20000)\n",
    "    # 평가 입실론 그리디 탐험 지정 람다 함수\n",
    "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "    # 재현 버퍼 지정 람다 함수\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n",
    "    \n",
    "    n_warmup_batches = 5             # 워밍업 배치 크기\n",
    "    update_target_every_steps = 10   # 타겟 신경망 갱신 스텝 수\n",
    "\n",
    "    # 환경 파라미터 지정 및 환경 생성\n",
    "    env_name, gamma, max_minutes, max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # DQN 에이전트 생성\n",
    "    agent = DQN(replay_buffer_fn,\n",
    "                value_model_fn,\n",
    "                value_optimizer_fn,\n",
    "                value_optimizer_lr,\n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                n_warmup_batches,\n",
    "                update_target_every_steps)\n",
    "    \n",
    "    # DQN 신경망 훈련\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(env, seed, gamma, max_minutes, \n",
    "                                                                                  max_episodes, goal_mean_100_reward)     \n",
    "    dqn_results.append(result)     # 시드 결과 리스트에 추가\n",
    "\n",
    "# 시드 결과 리스트를 넘파이 배열로 변환\n",
    "dqn_results = np.array(dqn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후 카트폴 동작 렌더링\n",
    "agent.render_after_train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efde82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래픽용 자료 추출\n",
    "dqn_max_t, dqn_max_r, dqn_max_s, \\\n",
    "    dqn_max_sec, dqn_max_rt = np.max(dqn_results, axis=0).T\n",
    "dqn_min_t, dqn_min_r, dqn_min_s, \\\n",
    "    dqn_min_sec, dqn_min_rt = np.min(dqn_results, axis=0).T\n",
    "dqn_mean_t, dqn_mean_r, dqn_mean_s, \\\n",
    "    dqn_mean_sec, dqn_mean_rt = np.mean(dqn_results, axis=0).T\n",
    "#dqn_x = np.arange(np.max((len(dqn_mean_s), 2200)))\n",
    "dqn_x =2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27d420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 에피소드 당 (마지막 100스탭) 평균 보상(훈련,평가), 총 스텝 수, 훈련/경과 시간 그래픽\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "# 그래픽 파라미터 설정\n",
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "# 서브그래프 분할\n",
    "fig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n",
    "\n",
    "# 마지막 100 스텝 훈련 시 평균 보상\n",
    "axs[0].plot(dqn_max_r, 'b', linewidth=1)\n",
    "axs[0].plot(dqn_min_r, 'b', linewidth=1)\n",
    "axs[0].plot(dqn_mean_r, 'b--', label='DQN', linewidth=2)\n",
    "axs[0].fill_between(dqn_x, dqn_min_r, dqn_max_r, facecolor='b', alpha=0.3)\n",
    "# 마지막 100 스텝 평가 시 평균 보상\n",
    "axs[1].plot(dqn_max_s, 'b', linewidth=1)\n",
    "axs[1].plot(dqn_min_s, 'b', linewidth=1)\n",
    "axs[1].plot(dqn_mean_s, 'b--', label='DQN', linewidth=2)\n",
    "axs[1].fill_between(dqn_x, dqn_min_s, dqn_max_s, facecolor='b', alpha=0.3)\n",
    "# 총 타임 스텝 수\n",
    "axs[2].plot(dqn_max_t, 'b', linewidth=1)\n",
    "axs[2].plot(dqn_min_t, 'b', linewidth=1)\n",
    "axs[2].plot(dqn_mean_t, 'b--', label='DQN', linewidth=2)\n",
    "axs[2].fill_between(dqn_x, dqn_min_t, dqn_max_t, facecolor='b', alpha=0.3)\n",
    "# 훈련 시간\n",
    "axs[3].plot(dqn_max_sec, 'b', linewidth=1)\n",
    "axs[3].plot(dqn_min_sec, 'b', linewidth=1)\n",
    "axs[3].plot(dqn_mean_sec, 'b--', label='DQN', linewidth=2)\n",
    "axs[3].fill_between(dqn_x, dqn_min_sec, dqn_max_sec, facecolor='b', alpha=0.3)\n",
    "# 경과 시간\n",
    "axs[4].plot(dqn_max_rt, 'b', linewidth=1)\n",
    "axs[4].plot(dqn_min_rt, 'b', linewidth=1)\n",
    "axs[4].plot(dqn_mean_rt, 'b--', label='DQN', linewidth=2)\n",
    "axs[4].fill_between(dqn_x, dqn_min_rt, dqn_max_rt, facecolor='b', alpha=0.3)\n",
    "\n",
    "# 차트 제목\n",
    "axs[0].set_title('DQN: Moving Avg Reward (Training)')\n",
    "axs[1].set_title('DQN: Moving Avg Reward (Evaluation)')\n",
    "axs[2].set_title('DQN: Total Steps')\n",
    "axs[3].set_title('DQN: Training Time')\n",
    "axs[4].set_title('DQN: Wall-clock Time')\n",
    "\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
